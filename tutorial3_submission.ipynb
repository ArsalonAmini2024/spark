{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession # type: ignore\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()\n",
    "# spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose:\n",
    "1. Dropping Columns\n",
    "2. Dropping Rows\n",
    "3. Various Parameter In Dropping functionalities\n",
    "4. Handling Missing values by Mean, MEdian And Mode\n",
    "5. Updating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n",
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read dataset into pySpark\n",
    "df_pyspark = spark.read.csv('test2.csv',header=True,inferSchema=True)\n",
    "df_pyspark.printSchema()\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  31|        10| 30000|\n",
      "|  30|         8| 25000|\n",
      "|  29|         4| 20000|\n",
      "|  24|         3| 20000|\n",
      "|  21|         1| 15000|\n",
      "|  23|         2| 18000|\n",
      "|NULL|      NULL| 40000|\n",
      "|  34|        10| 38000|\n",
      "|  36|      NULL|  NULL|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the name column\n",
    "\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "|     NULL| 36|      NULL|  NULL|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop NA columns\n",
    "df_pyspark.na.drop().show()\n",
    "df_pyspark.na.drop(how=\"any\").show() # any row containing 1 at least one null value will be dropped\n",
    "df_pyspark.na.drop(how=\"any\",thresh=3).show() # any row containing 3 or more non-null values, won't drop it\n",
    "df_pyspark.na.drop(how=\"any\",subset=['Age']).show() # Drops any row in DF where Age column contains Null or NaN (only considers age column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+--------------+------+\n",
      "|          Name|           age|    Experience|Salary|\n",
      "+--------------+--------------+--------------+------+\n",
      "|         Krish|            31|            10| 30000|\n",
      "|     Sudhanshu|            30|             8| 25000|\n",
      "|         Sunny|            29|             4| 20000|\n",
      "|          Paul|            24|             3| 20000|\n",
      "|        Harsha|            21|             1| 15000|\n",
      "|       Shubham|            23|             2| 18000|\n",
      "|        Mahesh|Missing Values|Missing Values| 40000|\n",
      "|Missing Values|            34|            10| 38000|\n",
      "|Missing Values|            36|Missing Values|  NULL|\n",
      "+--------------+--------------+--------------+------+\n",
      "\n",
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col # type: ignore\n",
    "\n",
    "# Convert non-nullable columns to nullable strings first or it won't work \n",
    "df_pyspark = df_pyspark.withColumn(\"Experience\", col(\"Experience\").cast(\"string\"))\n",
    "df_pyspark = df_pyspark.withColumn(\"age\", col(\"age\").cast(\"string\"))\n",
    "\n",
    "\n",
    "# Fill missing values\n",
    "df_pyspark.na.fill('Missing Values',['Name','Experience','age']).show() #Replaces with string 'missing values' for columns [name, experience, age]\n",
    "\n",
    "df_pyspark.show() #original data frame is not affected by the previous method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: What's goig on here? I thought we removed those rows. Explain here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The na.drop() method returns a new DataFrame with rows containing missing values removed, it doesn't modify the original dataFrame in place.\n",
    "\n",
    "you'd have to do this...\n",
    "\n",
    "df_pyspark = df_pyspark.na.drop() to re-assign the variable 'df_pyspark' to the new array that was returned after dropping the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- Experience: double (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert columns to numeric data type\n",
    "df_pyspark = df_pyspark.withColumn(\"age\", col(\"age\").cast(\"double\"))\n",
    "df_pyspark = df_pyspark.withColumn(\"Experience\", col(\"Experience\").cast(\"double\"))\n",
    "df_pyspark = df_pyspark.withColumn(\"Salary\", col(\"Salary\").cast(\"double\"))\n",
    "\n",
    "df_pyspark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Return an imputer that will imput the mean for age, experience and salary for Null or NaN values\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "    ).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+-------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience| Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+-------+-----------+------------------+--------------+\n",
      "|    Krish|31.0|      10.0|30000.0|       31.0|              10.0|       30000.0|\n",
      "|Sudhanshu|30.0|       8.0|25000.0|       30.0|               8.0|       25000.0|\n",
      "|    Sunny|29.0|       4.0|20000.0|       29.0|               4.0|       20000.0|\n",
      "|     Paul|24.0|       3.0|20000.0|       24.0|               3.0|       20000.0|\n",
      "|   Harsha|21.0|       1.0|15000.0|       21.0|               1.0|       15000.0|\n",
      "|  Shubham|23.0|       2.0|18000.0|       23.0|               2.0|       18000.0|\n",
      "|   Mahesh|NULL|      NULL|40000.0|       29.0|               4.0|       40000.0|\n",
      "|     NULL|34.0|      10.0|38000.0|       34.0|              10.0|       38000.0|\n",
      "|     NULL|36.0|      NULL|   NULL|       36.0|               4.0|       20000.0|\n",
      "+---------+----+----------+-------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/02 03:48:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|  3000|\n",
      "|     Anna|    Rose|     F|  4100|\n",
      "|   Robert|Williams|     M|  6200|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [('James','Smith','M',3000), ('Anna','Rose','F',4100),\n",
    "  ('Robert','Williams','M',6200)\n",
    "]\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below PySpark code updates the salary column value of DataFrame by multiplying salary by 3 times.\n",
    "\n",
    "Note that withColumn() is used to update or add a new column to the DataFrame, when you pass the existing column name to the first argument to withColumn() operation it updates it , if the value is new then it creates a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|  9000|\n",
      "|     Anna|    Rose|     F| 12300|\n",
      "|   Robert|Williams|     M| 18600|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.withColumn(\"salary\", df.salary*3)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update Column Based on Condition\n",
    "\n",
    "Update a column value based on a condition by using When Otherwise. The example below updates the gender column with a value Male for M, Female for F and keeps the same value for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|  Male|  3000|\n",
      "|     Anna|    Rose|Female|  4100|\n",
      "|   Robert|Williams|  Male|  6200|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df3 = df.withColumn(\"gender\", when(df.gender == \"M\",\"Male\") \\\n",
    "      .when(df.gender == \"F\",\"Female\") \\\n",
    "      .otherwise(df.gender))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update DataFrame Column Data Type\n",
    "\n",
    "You can also update a Data Type of a column using withColumn(). In addition you have to use the cast() function of the PySpark Column class. The code below updates salary column to String type.\n",
    "\n",
    "PySpark SQL Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+\n",
      "|firstname|gender|salary|\n",
      "+---------+------+------+\n",
      "|    James|     M|  9000|\n",
      "|     Anna|     F| 12300|\n",
      "|   Robert|     M| 18600|\n",
      "+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"PER\")\n",
    "df5=spark.sql(\"select firstname,gender,salary*3 as salary from PER\")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TODO: Use spark.sql to order the original dataframe by salary descending\n",
    "2. TODO: Use spark.sql to determine the average salary for each gender, order results by average salary descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|   Robert|Williams|     M|  6200|\n",
      "|     Anna|    Rose|     F|  4100|\n",
      "|    James|   Smith|     M|  3000|\n",
      "+---------+--------+------+------+\n",
      "\n",
      "+------+--------------+\n",
      "|gender|average_salary|\n",
      "+------+--------------+\n",
      "|     M|        4600.0|\n",
      "|     F|        4100.0|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first register as a temp SQL table\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "#run SQL query to achieve ordering\n",
    "ordered_df = spark.sql(\"SELECT * FROM people ORDER BY salary DESC\")\n",
    "ordered_df.show()\n",
    "\n",
    "# execute SQL query to calc average salary by gender and order descending\n",
    "average_salary_by_gender = spark.sql(\"\"\"\n",
    "SELECT gender, AVG(salary) AS average_salary\n",
    "FROM people\n",
    "GROUP BY gender\n",
    "ORDER BY average_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "average_salary_by_gender.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Agg').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark RDD – Resilient Distributed Dataset\n",
    "\n",
    "- PySpark RDD (Resilient Distributed Dataset) is a fundamental data structure of PySpark that is fault-tolerant, immutable distributed collections of objects, which means once you create an RDD you cannot change it. Each dataset in RDD is divided into logical partitions, which can be computed on different nodes of the cluster.\n",
    "- In order to create an RDD, we need to create a SparkSession which is an entry point to the PySpark application. SparkSession can be created using a builder() or newSession() methods of the SparkSession.\n",
    "- Spark session internally creates a sparkContext variable of SparkContext. We can create multiple SparkSession objects but only one SparkContext per JVM. In case if you want to create another new SparkContext you should stop existing Sparkcontext (using stop()) before creating a new one.\n",
    "- If you have tabular data, you should just use a DataFrame. Going down to the RDD layer is helpful for unstructured data, for example, text, images, and signal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from list using parallelize\n",
    "\n",
    "dataList = [(\"Java\", 2000), (\"Python\", 10000), (\"Scala\", 3000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from external data source\n",
    "\n",
    "dataList = [(\"Java\", 2000), (\"Python\", 10000), (\"Scala\", 3000)]\n",
    "rdd2=spark.sparkContext.textFile('test3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "RDD Transformations\n",
    "- Transformations on Spark RDD returns another RDD and transformations are lazy meaning they don’t execute until you call an action on RDD. Some transformations on RDD’s are flatMap(), map(), reduceByKey(), filter(), sortByKey() and return new RDD instead of updating the current.\n",
    "\n",
    "PySpark DataFrame\n",
    "- DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with optimizations under the hood. DataFrames can be constructed from a wide array of sources such as structured data files, tables in Hive, external databases, or existing RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Is PySpark faster than pandas? Explain your thinking.\n",
    "\n",
    "It depends on the scale of the data...\n",
    "- fits in memory on a single machine \n",
    "- is too large for a single machine and is distributed across multiple machines \n",
    "\n",
    "PySpark is faster when...\n",
    "- Large datasets: very large datasets that don't fit into a single machines memory can be paralellized across multiple machines in a cluster. This allows PySpark to process distributed large data outside of Pandas scope (memory limits)\n",
    "\n",
    "Pandas is faster when...\n",
    "- Small datasets: data that fits into memory on a single machine is processed using efficient operations for in-memory computation by pandas opposed to PySpark which has a lot of overhead due to distributed nature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv('test3.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show()\n",
    "df_pyspark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.createOrReplaceTempView(\"PEOPLE\")\n",
    "df1=spark.sql(\"select Name, Departments, Salary as salary from PEOPLE\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n",
      "+---------+------------+\n",
      "|     Name|Total_Salary|\n",
      "+---------+------------+\n",
      "|Sudhanshu|       35000|\n",
      "|    Sunny|       12000|\n",
      "|    Krish|       19000|\n",
      "|   Mahesh|        7000|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby\n",
    "\n",
    "# Grouped to find the maximum salary\n",
    "df_pyspark.groupBy('Name').sum().show()\n",
    "\n",
    "spark.sql(\"SELECT Name, SUM(Salary) as Total_Salary FROM PEOPLE GROUP BY Name\").show() # using PySpark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n",
      "+---------+------------------+\n",
      "|     Name|    Average_Salary|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n",
      "+------------+-----------+\n",
      "| Departments|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n",
      "+------------+------------+\n",
      "| Departments|Total_Salary|\n",
      "+------------+------------+\n",
      "|Data Science|       43000|\n",
      "|         IOT|       15000|\n",
      "|    Big Data|       15000|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using PySpark Dataframe\n",
    "df_pyspark.groupBy('Name').avg().show()\n",
    "\n",
    "# using PySpark SQL\n",
    "spark.sql(\"SELECT Name, AVG(Salary) as Average_Salary FROM PEOPLE GROUP BY Name\").show()\n",
    "\n",
    "# using PySpark Dataframe\n",
    "department_salary_sum = df_pyspark.groupBy(\"Departments\").sum(\"Salary\")\n",
    "department_salary_sum.show()\n",
    "\n",
    "# using PySpark SQL\n",
    "department_salary_sum = spark.sql(\"\"\"\n",
    "SELECT Departments, SUM(Salary) as Total_Salary\n",
    "FROM PEOPLE\n",
    "GROUP BY Departments\n",
    "ORDER BY Total_Salary DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "department_salary_sum.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n",
      "+------------+----------------+\n",
      "| Departments|Department_Count|\n",
      "+------------+----------------+\n",
      "|         IOT|               2|\n",
      "|    Big Data|               4|\n",
      "|Data Science|               4|\n",
      "+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using PySpark Dataframe\n",
    "department_count = df_pyspark.groupBy(\"Departments\").count()\n",
    "department_count.show()\n",
    "\n",
    "# using PySpark SQL\n",
    "spark.sql(\"SELECT Departments, COUNT(*) as Department_Count FROM PEOPLE GROUP BY Departments\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
